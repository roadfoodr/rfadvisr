import streamlit as st

import os
import yaml
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
import datetime
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
import re
from pathlib import Path

# Constants
EDITION = '10th'

# Set up Streamlit page configuration - MUST be the first Streamlit command
st.set_page_config(
    page_title=f"Roadfood {EDITION} Edition Search",
    page_icon="üçΩÔ∏è",
    layout="wide"
)

# OPENAI API SETUP
# Check if running in Modal (environment variable will be set) or locally (use credentials.yml)
if "OPENAI_API_KEY" not in os.environ:
    try:
        # Running locally, load from credentials.yml
        os.environ['OPENAI_API_KEY'] = yaml.safe_load(open('credentials.yml'))['openai']
    except Exception as e:
        st.error(f"Error loading OpenAI API key: {str(e)}")
        st.stop()

MODEL_EMBEDDING = 'text-embedding-ada-002'
LLM_MODEL = 'gpt-3.5-turbo'
# LLM_MODEL = 'gpt-4o-mini'

# Initialize embedding function
@st.cache_resource
def get_embedding_function():
    return OpenAIEmbeddings(
        model=MODEL_EMBEDDING,
    )

# Initialize LLM
@st.cache_resource
def get_llm():
    """Get the base LLM model"""
    # Always return the base model
    return ChatOpenAI(model=LLM_MODEL, temperature=0.7)

# Determine the base directory for data files
def get_base_dir():
    """Get the base directory for data files based on environment"""
    # Check if running in Modal
    if os.path.exists("/root/data"):
        return Path("/root")
    else:
        return Path(".")

# Load the existing Chroma database
@st.cache_resource
def get_vectorstore():
    embedding_function = get_embedding_function()
    base_dir = get_base_dir()
    persist_directory = base_dir / f"data/chroma_rf{EDITION}"
    return Chroma(
        embedding_function=embedding_function,
        persist_directory=str(persist_directory)
    )

# Get cached resources
embedding_function = get_embedding_function()
# llm = get_llm() # Remove this global instance, it's not used
vectorstore = get_vectorstore()

def post_process_summary(summary_text, search_results):
    """
    Post-process the summary to bold all restaurant names and add hyperlinks to their first occurrence.
    
    Args:
        summary_text: The text of the summary generated by the LLM
        search_results: The search results containing restaurant metadata
    
    Returns:
        Processed summary text with formatting applied
    """
    # Extract restaurant names and URLs from search results
    restaurants = []
    for doc in search_results:
        # Extract metadata if available
        metadata = getattr(doc, 'metadata', {})
        restaurant_name = metadata.get('Restaurant', '')
        url = metadata.get('URL', '')
        
        # Only process if we have a restaurant name
        if restaurant_name:
            restaurants.append({
                'name': restaurant_name,
                'url': url
            })
    
    # Track first occurrences
    first_occurrence = {r['name']: True for r in restaurants}
    
    # Sort restaurant names by length (descending) to avoid partial replacements
    # (e.g., replace "Joe's Diner" before "Joe")
    restaurants.sort(key=lambda x: len(x['name']), reverse=True)
    
    # Process each restaurant name
    for restaurant in restaurants:
        name = restaurant['name']
        url = restaurant['url']
        
        # Skip empty names
        if not name.strip():
            continue
        
        # Create regex pattern to match whole words/phrases only
        # This avoids replacing parts of other words
        # Use word boundaries \b for single-word restaurant names
        if len(name.split()) == 1:
            # For single words, use word boundaries
            pattern = r'(?<!\*\*)\b' + re.escape(name) + r'\b(?!\*\*)'
        else:
            # For phrases, use the existing pattern
            pattern = r'(?<!\*\*)' + re.escape(name) + r'(?!\*\*)'
        
        # Check if the name appears in the text
        if re.search(pattern, summary_text, re.IGNORECASE):
            # For the first occurrence, add hyperlink (if URL exists) and bold
            if first_occurrence[name]:
                if url:
                    # Add https:// prefix if missing
                    if not url.startswith('http'):
                        url = f"https://{url}"
                    # Replace with hyperlinked and bolded version
                    replacement = f"[**{name}**]({url})"
                else:
                    # Just bold if no URL
                    replacement = f"**{name}**"
                
                # Replace only the first occurrence (case insensitive)
                summary_text = re.sub(pattern, replacement, summary_text, count=1, flags=re.IGNORECASE)
                first_occurrence[name] = False
                
                # Bold all subsequent occurrences
                # Use the same pattern for consistency
                summary_text = re.sub(pattern, f"**{name}**", summary_text, flags=re.IGNORECASE)
            
    return summary_text

def standardize_summary_headline(summary_text):
    """
    Standardize headline formatting in summaries to ensure consistent heading levels.
    
    This function:
    1. Identifies the title (content before the bullet list)
    2. Converts the title to an h3 header, removing any formatting or prefixes
    3. Leaves the rest of the content unchanged
    
    Args:
        summary_text: The text of the summary to process
        
    Returns:
        Processed summary text with standardized headings
    """
    # Split the text into lines for processing
    lines = summary_text.split('\n')
    
    # Skip any empty lines at the beginning
    start_index = 0
    while start_index < len(lines) and not lines[start_index].strip():
        start_index += 1
    
    # If we've reached the end, return the original text
    if start_index >= len(lines):
        return summary_text
    
    # Find the first bullet point or empty line after non-empty content
    bullet_index = -1
    for i in range(start_index + 1, len(lines)):
        line = lines[i].strip()
        # If this is a bullet point, we've found our separator
        if line.startswith('*') or line.startswith('-'):
            bullet_index = i
            break
        # If this is an empty line after we've seen content, it might be our separator
        if not line and i > start_index:
            # Check if the next line exists and is not empty (to avoid treating paragraph breaks as separators)
            if i + 1 < len(lines) and lines[i + 1].strip():
                bullet_index = i
                break
    
    # If we didn't find a bullet point or suitable empty line, just use the first line as the title
    if bullet_index == -1:
        # Extract and clean the first non-empty line as the title
        title = lines[start_index].strip()
        # Remove any markdown headers
        title = re.sub(r'^#+\s+', '', title)
        # Remove bold markers
        title = re.sub(r'\*\*', '', title)
        # Remove common title prefixes
        title = re.sub(r'^(?:Title:|Catchy Title:|Headline:)\s*', '', title, flags=re.IGNORECASE)
        
        # Replace the first line with the cleaned h3 title
        if title:  # Only if we have actual content
            lines[start_index] = f"### {title}"
        
        # Convert any other h1 or h2 headers to h3
        for i in range(start_index + 1, len(lines)):
            if lines[i].strip().startswith('# ') or lines[i].strip().startswith('## '):
                lines[i] = re.sub(r'^#+\s+', '### ', lines[i])
        
        return '\n'.join(lines)
    
    # We found a separator, so extract everything before it as the title
    title_lines = [line for line in lines[start_index:bullet_index] if line.strip()]
    
    # If we have title lines, process them
    if title_lines:
        # Join all title lines into a single string
        title_text = ' '.join([line.strip() for line in title_lines])
        
        # Clean up the title
        # Remove any markdown headers
        title_text = re.sub(r'^#+\s+', '', title_text)
        # Remove bold markers
        title_text = re.sub(r'\*\*', '', title_text)
        # Remove common title prefixes
        title_text = re.sub(r'^(?:Title:|Catchy Title:|Headline:)\s*', '', title_text, flags=re.IGNORECASE)
        
        # Create the new title as an h3 header
        new_title = f"### {title_text.strip()}"
        
        # Build the new content with the standardized title
        result = [new_title, '']  # Title followed by blank line
        result.extend(lines[bullet_index:])  # Add the rest unchanged
        
        return '\n'.join(result)
    
    # If we didn't find any title content, return the original text
    return summary_text

def load_prompt_template(prompt_type="basic"):
    """Load the prompt template from file
    
    Args:
        prompt_type: Type of prompt to load ("basic" or "advanced")
    
    Returns:
        The prompt template text or None if there was an error
    """
    base_dir = get_base_dir()
    filename = base_dir / f"prompts/{prompt_type}_summary_prompt.txt"
    try:
        with open(filename, "r") as f:
            return f.read()
    except Exception as e:
        st.error(f"Error reading prompt template {filename}: {str(e)}")
        return None

def reload_prompt_template():
    """Clear the cache to reload the prompt template"""
    # Clear the cache to force reload of prompt
    st.cache_data.clear()
    st.success("Prompt templates reloaded!")

def generate_summary(query, full_content, search_results):
    """Generate a summary article from the search results using the advanced prompt and base LLM
    
    Args:
        query: The search query
        full_content: The full content of the search results
        search_results: The search results objects
        
    Returns:
        The generated summary
    """
    # Read the ADVANCED prompt template from file
    prompt_text = load_prompt_template("advanced")
    
    # Fallback to hardcoded prompt if file can't be read
    if not prompt_text:
        # Simplified fallback prompt if needed (though ideally the file load works)
        prompt_text = """
        You are a food writer creating a detailed summary article based on search results for "{query}".
        
        Here are the details of restaurants found in the search:
        {full_content}
                
        Format your response with markdown, including a title and bullet points.
        """
    
    # Create the prompt for the LLM
    prompt_template = ChatPromptTemplate.from_template(prompt_text)
    
    # Get the base model
    model = get_llm() 
    
    # Generate the summary
    chain = prompt_template | model
    response = chain.invoke({"query": query, "full_content": full_content})
    
    # Post-process the summary to format restaurant names
    processed_summary = post_process_summary(response.content, search_results)
    
    return processed_summary

@st.cache_data
def perform_search(query, num_results):
    """Perform the vector search and return results"""
    if not query.strip():
        return []
    
    try:
        results = vectorstore.similarity_search(
            query=query,
            k=num_results
        )
        return results
    except Exception as e:
        st.error(f"Error during search: {str(e)}")
        return []

def prepare_download_content(query, content):
    """Prepare content for download without saving to disk"""
    # Format the content for download
    formatted_content = f"Search query: {query}\n\n{content}"
    return formatted_content

def prepare_download_content_for_summaries(query, summary_text):
    """Prepare summary content for download
    
    Args:
        query: The search query
        summary_text: The generated summary text
        
    Returns:
        Formatted content for download
    """
    content = f"Search query: {query}\n\n"
    content += "SUMMARY:\n\n" # Simplified header
    content += summary_text
    return content

def display_summary(summary_text):
    """Display the generated summary
    
    Args:
        summary_text: The generated summary text to display
    """
    # Display the single summary directly
    st.markdown(standardize_summary_headline(summary_text))

# Create Streamlit interface
st.title(f"Roadfood {EDITION} Edition Restaurant Search")
st.markdown("Search for restaurants based on your preferences, cuisine, location, etc.")

# Example queries outside the form (these don't trigger re-renders)
example_queries = [
    "Where is the best BBQ?",
    "Unique seafood restaurants on the East Coast",
    "Famous diners in New Jersey",
    "Where can I find good pie?",
    "Historic restaurants with great burgers"
]

# Store the selected example in session state so we can use it in the form
if 'selected_example' not in st.session_state:
    st.session_state.selected_example = ""

def update_example():
    if st.session_state.example_selector:
        st.session_state.selected_example = st.session_state.example_selector

# Example selector outside the form
st.sidebar.header("Example Searches")
st.sidebar.selectbox(
    "Try an example search:",
    [""] + example_queries,
    key="example_selector",
    on_change=update_example
)

# Create a form for all search inputs
with st.sidebar:
    with st.form(key="search_form"):
        st.header("Search Options")
        
        # Search parameters
        query_input = st.text_area(
            "What are you looking for?",
            value=st.session_state.selected_example,
            placeholder="e.g., 'best BBQ in Texas' or 'unique seafood restaurants'"
        )
        
        num_results = st.slider(
            "Number of results",
            min_value=1,
            max_value=10,
            value=4,
            step=1
        )
        
        generate_article_checkbox = st.checkbox("Generate summary article", value=True)
        
        save_checkbox = st.checkbox("Enable download option", value=False)
        
        # Form submit button
        search_submitted = st.form_submit_button("Search")

# Main content area - only process when form is submitted
if search_submitted:
    if not query_input.strip():
        st.warning("Please enter a search query.")
    else:
        with st.spinner("Searching for restaurants..."):
            # Perform the search
            search_results = perform_search(query_input, num_results)
            
            if search_results:
                # Process and display results based on user preferences
                if generate_article_checkbox:
                    # Extract full content for summarization
                    full_content = "\n\n".join([doc.page_content for doc in search_results])
                    
                    # Generate the single summary (advanced prompt, base model)
                    with st.spinner("Generating summary..."): 
                        summary_result = generate_summary(query_input, full_content, search_results)
                    
                    # Display the summary
                    display_summary(summary_result)
                    
                    # Save summary if requested
                    if save_checkbox:
                        download_content = prepare_download_content_for_summaries(query_input, summary_result)
                        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"search_results_summary_{timestamp}.txt"
                        st.download_button(
                            label="Download Summary",
                            data=download_content,
                            file_name=filename,
                            mime="text/plain"
                        )
                else:
                    # Display detailed results
                    output = []
                    for i, doc in enumerate(search_results):
                        output.append(f"## Result {i+1}:\n\n{doc.page_content}\n\n---")
                    
                    display_content = "\n".join(output)
                    
                    # Save to file if requested
                    if save_checkbox:
                        detailed_content = f"Search query: {query_input}\n\n"
                        for i, doc in enumerate(search_results):
                            detailed_content += f"Result {i+1}:\n"
                            detailed_content += f"{doc.page_content}\n"
                            detailed_content += "-" * 50 + "\n\n"
                        
                        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"search_results_{timestamp}.txt"
                        st.download_button(
                            label="Download Results",
                            data=detailed_content,
                            file_name=filename,
                            mime="text/plain"
                        )
                    
                    st.markdown(display_content)

# Display some information about the app
with st.expander("About this app"):
    st.markdown(f"""
    This app searches through the Roadfood database ({EDITION} edition) to find restaurants matching your criteria using vector embeddings.
    
    When the 'Generate summary article' option is checked, it creates a detailed summary using the search results.
    
    The summary includes restaurant names, locations, and highlights what makes them special, based on an advanced prompt template.
    """)

# Add developer options in a collapsed expander at the bottom of the sidebar
with st.sidebar:
    with st.expander("Developer Options", expanded=False):
        if st.button("Reload Prompt Template"):
            reload_prompt_template()

# Run the app
# Note: No need for if __name__ == "__main__" in Streamlit
# Streamlit apps are run with the command:  python -m streamlit run roadfood_search_app.py